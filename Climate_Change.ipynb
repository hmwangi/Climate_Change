{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "train_original=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8530\n",
       " 2    3640\n",
       " 0    2353\n",
       "-1    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @ezlusztig: They took down the material on ...</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @washingtonpost: How climate change could b...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>notiven: RT: nytimesworld :What does Trump act...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @sara8smiles: Hey liberals the climate chan...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @Chet_Cannon: .@kurteichenwald's 'climate c...</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "0              1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1              1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2              2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3              1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4              1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
       "...          ...                                                ...      ...\n",
       "15814          1  RT @ezlusztig: They took down the material on ...    22001\n",
       "15815          2  RT @washingtonpost: How climate change could b...    17856\n",
       "15816          0  notiven: RT: nytimesworld :What does Trump act...   384248\n",
       "15817         -1  RT @sara8smiles: Hey liberals the climate chan...   819732\n",
       "15818          0  RT @Chet_Cannon: .@kurteichenwald's 'climate c...   806319\n",
       "\n",
       "[15819 rows x 3 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "test_original=test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = train.append(test,ignore_index=True,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    8530\n",
       " 2.0    3640\n",
       " 0.0    2353\n",
       "-1.0    1296\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26360</th>\n",
       "      <td>RT @BrittanyBohrer: Brb, writing a poem about ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>895714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26361</th>\n",
       "      <td>2016: the year climate change came home: Durin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>875167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26362</th>\n",
       "      <td>RT @loop_vanuatu: Pacific countries positive a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26363</th>\n",
       "      <td>RT @xanria_00018: You’re so hot, you must be t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>867455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26364</th>\n",
       "      <td>RT @chloebalaoing: climate change is a global ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>470892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26365 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message  sentiment  tweetid\n",
       "0      PolySciMajor EPA chief doesn't think carbon di...        1.0   625221\n",
       "1      It's not like we lack evidence of anthropogeni...        1.0   126103\n",
       "2      RT @RawStory: Researchers say we have three ye...        2.0   698562\n",
       "3      #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736\n",
       "4      RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954\n",
       "...                                                  ...        ...      ...\n",
       "26360  RT @BrittanyBohrer: Brb, writing a poem about ...        NaN   895714\n",
       "26361  2016: the year climate change came home: Durin...        NaN   875167\n",
       "26362  RT @loop_vanuatu: Pacific countries positive a...        NaN    78329\n",
       "26363  RT @xanria_00018: You’re so hot, you must be t...        NaN   867455\n",
       "26364  RT @chloebalaoing: climate change is a global ...        NaN   470892\n",
       "\n",
       "[26365 rows x 3 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221\n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103\n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562\n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736\n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove capitalization and make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['Tidy_Tweets'] = combine['message'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>Tidy_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt @rawstory: researchers say three years act ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>#todayinmaker# wired : 2016 pivotal year war c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt @soynoviodetodas: 2016, racist, sexist, cli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                         Tidy_Tweets  \n",
       "0  polyscimajor epa chief think carbon dioxide ma...  \n",
       "1    like lack evidence anthropogenic global warming  \n",
       "2  rt @rawstory: researchers say three years act ...  \n",
       "3  #todayinmaker# wired : 2016 pivotal year war c...  \n",
       "4  rt @soynoviodetodas: 2016, racist, sexist, cli...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation, Numbers, and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation_numbers(post):\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "\n",
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(remove_punctuation_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>Tidy_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor epa chief think carbon dioxide ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rt rawstory researchers say three years act cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired   pivotal year war climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>rt soynoviodetodas  racist sexist climate chan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                         Tidy_Tweets  \n",
       "0  polyscimajor epa chief think carbon dioxide ma...  \n",
       "1    like lack evidence anthropogenic global warming  \n",
       "2  rt rawstory researchers say three years act cl...  \n",
       "3  todayinmaker wired   pivotal year war climate ...  \n",
       "4  rt soynoviodetodas  racist sexist climate chan...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "director climate change denier coal lobbies deputy chief environment\n",
      "irisrimon chinese broke massive iceberg antarctica part global warming hoax\n",
      "thinkprogress tillersons climate change emails\n",
      "mthgd global warming real club penguin shut\n",
      "ndenicolamd doctors agree climate change making sick earthday marchforscience healthandclimate\n",
      "tristinc president think climate change hoax made chinese vice president thinks curable disease\n",
      "resevoirs methane resevoirs play substantial role global warming\n",
      "fruitloopian snow march global warming\n",
      "greenharvard universities uniquely important role play battle climate change\n",
      "sethmacfarlane looking america officially believes climate change hoax sorry everybody else\n",
      "popsci ugliest animals threatened climate change\n",
      "realdonaldtrump potus fucking idiot geologist studied climate change college coauthored papers real\n",
      "sensanders presidentelect believe climate change millions people going\n",
      "someone explain march global warming\n",
      "natgeochannel watch beforetheflood right here leodicaprio travels world tackle climate change\n",
      "peta meat production leading cause climate change water waste deforestation extinction worldveganday\n",
      "explain global warming advocates\n",
      "syfys incorporated imagines future ravaged climate change\n",
      "prisonplanet finally climate change evil wake good\n",
      "drishtiias audio article environment trumps decision form paris climate change agreement\n",
      "nytnational white house budget proposal climate change were spending money anymore\n",
      "tomilahren kinda funny professors talk shit trumpstersomg them scientist climate change trumpserst idots kinda shows base\n",
      "stephenschlegel thinking going husband believe climate change\n",
      "sethmacfarlane proposes installing half billion solar panels first term trump thinks climate change hoax\n",
      "lexiprez literally climate change denierrip planet\n",
      "realdonaldtrump climate change real care\n",
      "kloppholic imagine trying convince global warming real instead conspiracy wake\n",
      "docrock saying climate change hoax every major scientific organization\n",
      "politicalshort piewagn dems causing global warming stfu\n",
      "signordal global warming study terrible news alarmists good news plants animals\n",
      "follow projectarcc want hear digital archivists climate change activism avhack\n",
      "believe climate change look polar bears going extinct land live\n",
      "pollution deaths expected rise climate change\n",
      "labour budgeting climate change punch climatechange\n",
      "hear antivaccersclimate change deniersaca killers stop using stupidity\n",
      "reader questions science climate change theories climatechange\n",
      "impacts global warming\n",
      "avert climate change world apply method\n",
      "foxandfriends catholic priests expected preach global warming congregations report says foxnation\n",
      "sasjabeslik meet chinas ecological migrants people displaced climate change\n",
      "china warns trump abandoning climate change deal\n",
      "science rescue climate change threatens chocolate europe\n",
      "stephenschlegel thinking going husband believe climate change\n",
      "zachheltzel people believe climate change also generally refuse trans real loud better\n",
      "samjamesvelde degrees early november angeles know according trump global warming climate change\n",
      "come hear climate change antarctica artclimate larsenc climatechangeart\n",
      "youths almost kenyas best line defense climate change food security robertmburia richardmunang\n",
      "cbsnews going away this going call every single time pages gone fake climate change\n",
      "geographerjay trump wants nasa climate change research real estate markets start slump climate change\n",
      "charliedaniels worried global warming terrified global government\n",
      "jeffdsachs panelists erupt climate change video time call lies trump koch brothers heritage\n",
      "nasajohnson explain climate change\n",
      "jinjjarevil seokjin blinks adorable global warming stops trees grow crime rate decreases\n",
      "worried planet trump nothing climate change\n",
      "effect methane climate change could greater thought physorgcom\n",
      "astrokatie governments several world powers failing climate change need without want hope\n",
      "benhulac asked charges exxonmobil withheld information climate change tillerson declines answer twice secretaryofst\n",
      "kamalaharris stand jerrybrowngov saying california back climate change\n",
      "steveengland considered piece column steve inform impact climate change species habitats\n",
      "rjsalmond already climate change denial either would loss well\n",
      "quiz cultural heritageclimate change disclaimer agree answers still interesting\n",
      "benwikler worry natural process anthropogenic climate change rendering earth uninhabitable\n",
      "look whos back town thank global warming enjoying quick trip brown county state park degree november\n",
      "alanresnicks horsemen apocalypse famine death pestilence global warming\n",
      "global warming hoax perpetrated chinese\n",
      "safetypindaily federal department censoring term climate change emails reveal olliemilman\n",
      "puffnpuffin stevemartintogo billnye bumfonline could someone link journal reviewed article climate change isnt\n",
      "guess global warming thank\n",
      "hawaiidelilah trump went ivanka reporter insights climate change scientists women expertis\n",
      "conservative columnist siege times debut climate change news\n",
      "pakistan vulnerable country terms climate change whats experienced prime minister about\n",
      "mains global warming\n",
      "indyusa trumps budget director said combating climate change waste money\n",
      "knock brexit trump think climate change bury issuesbut really hope\n",
      "world climate change drop dead nobel scientist ignorance shocking\n",
      "trudeau must emphasis defence wants trump onside trade climate change nationalpost\n",
      "carbongate physicist cause climate change responds video\n",
      "lastweektonight dont believe manmade global warming silly issue give natural resources defense council\n",
      "thehill rahm emanuel posts climate change webpage deleted trump administration\n",
      "jackposobiec soon altright blamed global warming gingivitis\n",
      "deanofcomedy followed closely climate change deniers wall street executives\n",
      "agreed manmade climate change manmade hoax innit\n",
      "call take urgent action combat climate change impacts join movement\n",
      "bradleym juddlegum believe climate change scientific fact matte\n",
      "stephenschlegel thinking going husband believe climate change\n",
      "tamorley zekejmiller carbon climate change stuff hokum\n",
      "energydesk historic coal fall profound impact global efforts tackle climate change\n",
      "china calls donald trump climate change know\n",
      "literateliberal leading global warming deniers told want trump motherjones\n",
      "billmckibben exxonmobil long history peddling misinformation climate change elizkolbert newyorker exxonknew\n",
      "pablorodas climatechange west coast states fight climate change even trump\n",
      "ecointernet secretary state tillerson signs arctic agreement action climate change antinuclear\n",
      "stephen hawking message trump ignore climate change\n",
      "telesurenglish jamaicas iconic beaches vanishing thanks climate change\n",
      "mrdenmore apart taking brink recession doubling deficit making pariah refugees climate change\n",
      "mikebeacham scientist cooked climate change books ahead obama presentation nomorescams nomoredemocrats draintheswamp\n",
      "natgeochannel easiest ways help combat climate change daily life stop eating beef heres\n",
      "effects climate change\n",
      "marcuschown denier white house deal global warming catastrophe threatens billion with\n",
      "nzaegel effects climate change wreak havoc mental health doctors\n",
      "people argue climate change\n",
      "glblctzn hear trumps director says carbon cause climate change\n",
      "countermoonbat people predicted parts manhattan would underwater climate change concerned fake\n",
      "thinkprogress brace bitterly cold winter climate change shifts polar vortex\n",
      "senatormroberts much global warming finkel blueprint stop none would deindustrialise economy\n",
      "business china tells trump climate change hoax invented\n",
      "turnscoat cant agree climate change number priority\n",
      "prisionplaneta scottadamssays scientists studies global warming choose show ones like\n",
      "climatekeith saskatchewan provincial govt plan policies target address climate change skpol\n",
      "asked inspiration told global warming\n",
      "energydesk exxon shareholders moved force company disclose risk climate change poses business\n",
      "mobilizeclimate must stop treating effects climate change doomsday here\n",
      "think solved climate change problem told kids dont global warming back wait dadlife\n",
      "natureorg world leaders reaffirm commitment climate change parisagreement\n",
      "jacobwhitesides think thats global warming christmas\n",
      "ecointernet john roughan miss sceptical voice climate change zealand herald environment\n",
      "chejackson climate change real anything columbus right november realdonaldtrump\n",
      "tillerson refuses admit exxonknew climate change decades huffpostpol\n",
      "wakmax talk climate change says alicebell itll make feel better read\n",
      "duycks join tomorrow event childrenrights context climate change message protect empower\n",
      "unep boost funding developing countries adapt effects climate change issue\n",
      "paulhbeckwith listen folks story evil twin climate change oceans acid\n",
      "huffingtonpost china trump climate change chinese hoax cmdangelo\n",
      "businessinsider apple borrowing billion fight climate change\n",
      "stephenschlegel thinking going husband believe climate change\n",
      "bradydennis abruptly cancels longplanned conference climate change health\n",
      "tell story climate change story contest climatechange\n",
      "zrastall climate change fakenews\n",
      "popsci river canada turned piracy global warming\n",
      "mrbrexit good news realdonaldtrump looking ditch paris agreement bust open climate change myth maga stepfo\n",
      "adriangeolopez people still believe global warming especially president pulling\n",
      "please health fitness late reverse global warmingampquot scientists\n",
      "scholaurship motherfucker believe climate change\n",
      "petraau countries subtweeted trump global warming\n",
      "arizonapatriot joelpollak gabyendingstory least obama christianextremist thinks global warming evolution hoaxes\n",
      "thedailyclimate frogs heading uphill escape climate change india timesofindia\n",
      "climate change campaigner dies snorkeling greatbarrierreef greatbarrierreef\n",
      "lianamaeby well plus side climate change real anymore\n",
      "robydalvik kalem karena faktor global warming laut naik\n",
      "climatechangrr sends much smaller team climate talks bonn climate home climate change news\n",
      "stephenschlegel thinking going husband believe climate change\n",
      "conversation morning arthur cold global warming obviously fake agreed arthur whos\n",
      "natgeophotos although climate change scary topic insanely unimaginably beautiful\n",
      "realdonaldtrump want everyones president something progressive fight climate change protect\n",
      "exxon hero global warming kicked saving earth selected money\n",
      "impacts climate change include losses business interruptions worsening working cnvey\n",
      "sensanders presidentelect doesnt believe climate change thats frightening country world\n",
      "taylorandbrown think number thing bothers trump president going defund climate change\n",
      "washtimes climate change whistleblower alleges noaa manipulated data hide global warming pause washtimes noaa\n",
      "exxonmobil exxon tillerson reportedly used alias emails climate change read more\n",
      "illuminateboys cant believe politicians believe climate change fake people working\n",
      "jackposobiec global warming caused cabbage farts\n",
      "thinking thing goes climate change important issues effect amer\n",
      "simonwest brilliant cartoon sums realdonaldtrump decision cancel barackobama climate change policies itisreal\n",
      "jamespeshaw nzgreens gets poverty zealand gets clean water action climate change\n",
      "adam adamcurry maakte keer zijn agenda show leuk info global warming\n",
      "shadbase shadbase people believe global warming explain\n",
      "berniesanders imvotingbecause future planet stake hillary clinton combat climate change donald trump thinks\n",
      "uniocracy theyll tell theyre save global warming theyre lying opchemtrails\n",
      "conversationedu turtles around million years pace climate change existential challenge\n",
      "lawlib board bouncing ideas climate change interested\n",
      "erry real reason fight climate change ccol\n",
      "newsfakenews todd starnes take global warming nonsense kids blow tailpipe\n",
      "guardianeco stopping global warming save great barrier reef scientists warn greatbarrierreef\n",
      "nobamadotcom libbyrafferty bizpacreview funny left scream allegiance science global warming issue\n",
      "lisalsong another horrifying outcomes global warming\n",
      "rebuke trump policy immelt says climate change real\n",
      "stevevsninjas climate change thats data point like concluded white speak englis\n",
      "sierraclubwi pollutingpruitt thinks human impact climate change subject continued debate climatefacts\n",
      "countermoonbat people predicted parts manhattan would underwater climate change concerned fake\n",
      "thehill arnold schwarzenegger teams macron fight climate change\n",
      "yayitsrob trump twostep climate change something kinda moderate something extremely radical\n",
      "race curb climate change cities outpace governments parisagreement globalcovenant citiesclimate\n",
      "jellybeatles board bouncing ideas climate change interested\n",
      "climatehome week save climate change talks trump\n",
      "businessinsider trump lead climate change could make china great again guardian\n",
      "aleszubajak chile there space climate denial climate change threatening multiple shapes\n",
      "realdonaldtrump concept global warming created chinese order make manufacturing noncompetitive\n",
      "michaelawat much denial energy department climate office bans phrase climate change\n",
      "natural climate change wins manmade climate change\n",
      "classicgrrl nuclear annihilation climate change kill slowly\n",
      "weneedhillary evening news donald trump global warming skeptic charge protecting environment\n",
      "savingoceans wondering climate change impacts oceans check climatechange billnye\n",
      "harmlessyarddog kids aged suing trump climate change gtleftist disease\n",
      "stop global warming putting outside\n",
      "drbobbullard theres plan climate change scientists fear consequence trump victory\n",
      "francehillary trump chinese agent ignoring climate change benefits clean energy helps china weak\n",
      "mashable trump named climate change denier lead agency fights global warming\n",
      "kamalaharris proud represent state finding creative ways climate change\n",
      "palmerreport nodapl golden gate park joins badlands national park defying donald trump tweeting climate change\n",
      "ineedtakeatwit zwash foxnews thejuanwilliams lolololololol oceans rising even without climate change\n",
      "going creationists dont believe global warming never listened bill\n",
      "stand successes climate change\n",
      "fusion america become country world leader doesnt think global warming real\n",
      "hayleiip someone told qualified discuss climate change pulled degree environmental studies\n",
      "wingsscotland belfast alone probably accelerates global warming year night\n",
      "watching scott pruitt talk nonsense global warming raising fool parisagreement\n",
      "dicapriofdn preach global warming yacht hypocrite\n",
      "watched really need climate change going worse\n",
      "googlegirl sues indian government inaction climate change\n"
     ]
    }
   ],
   "source": [
    "for x in combine['Tidy_Tweets'][100:300]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()\n",
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>Tidy_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, chief, think, carbon, dioxide, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>[rawstory, researchers, three, years, climate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, pivotal, year, climate, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>[soynoviodetodas, racist, sexist, climate, cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                         Tidy_Tweets  \n",
       "0  [polyscimajor, chief, think, carbon, dioxide, ...  \n",
       "1  [like, lack, evidence, anthropogenic, global, ...  \n",
       "2  [rawstory, researchers, three, years, climate,...  \n",
       "3  [todayinmaker, wired, pivotal, year, climate, ...  \n",
       "4  [soynoviodetodas, racist, sexist, climate, cha...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(climate_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>Tidy_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, chief, think, carbon, dioxide, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>[rawstory, researcher, three, year, climate, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, pivotal, year, climate, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>[soynoviodetodas, racist, sexist, climate, cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                         Tidy_Tweets  \n",
       "0  [polyscimajor, chief, think, carbon, dioxide, ...  \n",
       "1  [like, lack, evidence, anthropogenic, global, ...  \n",
       "2  [rawstory, researcher, three, year, climate, c...  \n",
       "3  [todayinmaker, wired, pivotal, year, climate, ...  \n",
       "4  [soynoviodetodas, racist, sexist, climate, cha...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>Tidy_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625221</td>\n",
       "      <td>polyscimajor chief think carbon dioxide main c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126103</td>\n",
       "      <td>like lack evidence anthropogenic global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>698562</td>\n",
       "      <td>rawstory researcher three year climate change ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>573736</td>\n",
       "      <td>todayinmaker wired pivotal year climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466954</td>\n",
       "      <td>soynoviodetodas racist sexist climate change d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  sentiment  tweetid  \\\n",
       "0  PolySciMajor EPA chief doesn't think carbon di...        1.0   625221   \n",
       "1  It's not like we lack evidence of anthropogeni...        1.0   126103   \n",
       "2  RT @RawStory: Researchers say we have three ye...        2.0   698562   \n",
       "3  #TodayinMaker# WIRED : 2016 was a pivotal year...        1.0   573736   \n",
       "4  RT @SoyNovioDeTodas: It's 2016, and a racist, ...        1.0   466954   \n",
       "\n",
       "                                         Tidy_Tweets  \n",
       "0  polyscimajor chief think carbon dioxide main c...  \n",
       "1    like lack evidence anthropogenic global warming  \n",
       "2  rawstory researcher three year climate change ...  \n",
       "3     todayinmaker wired pivotal year climate change  \n",
       "4  soynoviodetodas racist sexist climate change d...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(' '.join)\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features from cleaned Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26362</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26363</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26364</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26365 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  990  991  992  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    0  ...    0    0    1   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    0    0    1   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "26360    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "26361    0    0    0    0    0    0    0    0    0    0  ...    0    0    2   \n",
       "26362    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "26363    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "26364    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       993  994  995  996  997  998  999  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "26360    0    0    0    0    0    0    0  \n",
       "26361    0    0    0    0    0    0    0  \n",
       "26362    0    0    0    0    0    0    0  \n",
       "26363    0    0    0    0    1    0    0  \n",
       "26364    0    0    0    0    0    0    0  \n",
       "\n",
       "[26365 rows x 1000 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=2, max_features=1000, stop_words='english')\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(combine['Tidy_Tweets'])\n",
    "\n",
    "df_bow = pd.DataFrame(bow.todense())\n",
    "\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26365 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  990  991  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "26360  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "26361  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "26362  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "26363  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "26364  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "            992  993  994  995  996       997  998  999  \n",
       "0      0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "1      0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "2      0.437471  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "3      0.534234  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "4      0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "...         ...  ...  ...  ...  ...       ...  ...  ...  \n",
       "26360  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "26361  0.627163  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "26362  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "26363  0.000000  0.0  0.0  0.0  0.0  0.768585  0.0  0.0  \n",
       "26364  0.000000  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "\n",
       "[26365 rows x 1000 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf=TfidfVectorizer(max_df=0.70, min_df=2,max_features=1000,stop_words='english')\n",
    "\n",
    "tfidf_matrix=tfidf.fit_transform(combine['Tidy_Tweets'])\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting our dataset into Training and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the features from Bag-of-Words for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow = bow[:15819]\n",
    "\n",
    "train_bow.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using features from TF-IDF for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_matrix = tfidf_matrix[:15819]\n",
    "\n",
    "train_tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['sentiment'],\n",
    "                                                                      test_size=0.3,\n",
    "                                                                      random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['sentiment'],\n",
    "                                                                              test_size=0.3,\n",
    "                                                                              random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing f1_score from sklearn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Log_Reg = LogisticRegression(random_state=11,solver='lbfgs', max_iter=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Features\n",
    "Fitting the Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=400, random_state=11)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Log_Reg.fit(x_train_bow,y_train_bow,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  2, ..., -1,  1,  2], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_bow = Log_Reg.predict(x_valid_bow)\n",
    "\n",
    "prediction_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5904303429630383"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_bow = f1_score(y_valid_bow, prediction_bow, average='macro')\n",
    "log_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features\n",
    "Fitting the Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=400, random_state=11)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Log_Reg.fit(x_train_tfidf,y_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, ..., 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tfidf = Log_Reg.predict(x_valid_tfidf)\n",
    "\n",
    "prediction_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5759683009107299"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_tfidf = f1_score(y_valid_tfidf, prediction_tfidf, average='macro')\n",
    "log_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bow = XGBClassifier(random_state=22,learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.9, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=22, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Fitting the XGBoost Model\n",
    "model_bow.fit(x_train_bow, y_train_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ..., -1,  1,  2], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = model_bow.predict(x_valid_bow)\n",
    "\n",
    "xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5998370725550446"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bow=f1_score(y_valid_bow,xgb, average='macro')\n",
    "xgb_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDIF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tfidf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the XGBoost model\n",
    "model_tfidf.fit(x_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Predicting the probabilities.\n",
    "xgb_tfidf=model_tfidf.predict(x_valid_tfidf)\n",
    "\n",
    "xgb_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5671245793108947"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the F1 Score\n",
    "score=f1_score(y_valid_tfidf,xgb_tfidf, average='macro')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dct = DecisionTreeClassifier(criterion='entropy', random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=11)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the Decision Tree model.\n",
    "dct.fit(x_train_bow,y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  2, ..., -1,  1,  2], dtype=int64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_bow = dct.predict(x_valid_bow)\n",
    "\n",
    "dct_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5311575880098476"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating f1 score\n",
    "dct_score_bow=f1_score(y_valid_bow,dct_bow, average='macro')\n",
    "\n",
    "dct_score_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=11)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the Decision Tree model\n",
    "dct.fit(x_train_tfidf,y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  0, ..., -1,  1,  2], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_tfidf = dct.predict(x_valid_tfidf)\n",
    "\n",
    "dct_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5291142125450711"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calculating the F1 Score\n",
    "dct_score_tfidf=f1_score(y_valid_tfidf,dct_tfidf, average ='macro')\n",
    "\n",
    "dct_score_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=50, n_estimators=800)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=800, max_depth=50,max_features='auto')\n",
    "# Bag of words\n",
    "rfc.fit(x_train_bow,y_train_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_prediction_bow = rfc.predict(x_valid_bow)\n",
    "\n",
    "rfc_prediction_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5305556761363619"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_bow = f1_score(y_valid_bow, rfc_prediction_bow, average ='macro')\n",
    "\n",
    "log_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>LogisticRegression(Bag-of-Words)</td>\n",
       "      <td>XGBoost(Bag-of-Words)</td>\n",
       "      <td>DecisionTree(Bag-of-Words)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_Score</th>\n",
       "      <td>0.530556</td>\n",
       "      <td>0.599837</td>\n",
       "      <td>0.531158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         1                      2  \\\n",
       "Model     LogisticRegression(Bag-of-Words)  XGBoost(Bag-of-Words)   \n",
       "F1_Score                          0.530556               0.599837   \n",
       "\n",
       "                                   3  \n",
       "Model     DecisionTree(Bag-of-Words)  \n",
       "F1_Score                    0.531158  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Algo_1 = ['LogisticRegression(Bag-of-Words)','XGBoost(Bag-of-Words)','DecisionTree(Bag-of-Words)']\n",
    "\n",
    "score_1 = [log_bow,xgb_bow,dct_score_bow]\n",
    "\n",
    "compare_1 = pd.DataFrame({'Model':Algo_1,'F1_Score':score_1},index=[i for i in range(1,4)])\n",
    "\n",
    "compare_1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>LogisticRegression(TF-IDF)</td>\n",
       "      <td>XGBoost(TF-IDF)</td>\n",
       "      <td>DecisionTree(TF-IDF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_Score</th>\n",
       "      <td>0.575968</td>\n",
       "      <td>0.567125</td>\n",
       "      <td>0.529114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   1                2                     3\n",
       "Model     LogisticRegression(TF-IDF)  XGBoost(TF-IDF)  DecisionTree(TF-IDF)\n",
       "F1_Score                    0.575968         0.567125              0.529114"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Algo_2 = ['LogisticRegression(TF-IDF)','XGBoost(TF-IDF)','DecisionTree(TF-IDF)']\n",
    "\n",
    "score_2 = [log_tfidf,score,dct_score_tfidf]\n",
    "\n",
    "compare_2 = pd.DataFrame({'Model':Algo_2,'F1_Score':score_2},index=[i for i in range(1,4)])\n",
    "\n",
    "compare_2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the results for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = bow[15819:]\n",
    "test_pred = model_bow.predict(test_tfidf)\n",
    "test['sentiment'] = test_pred\n",
    "submission = test[['tweetid','sentiment']]\n",
    "submission.to_csv('result_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
